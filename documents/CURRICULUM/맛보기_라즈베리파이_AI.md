# 맛보기 출장수업: 라즈베리파이 AI 비전 로봇카

## 🎯 Hero Section

**배지**: "하나의 교구재로 완성하는 AI 비전 개발"  
**타이틀**: "라즈베리파이 AI 비전 메이커"  
**설명**: "얼굴 인식부터 YOLO 자율주행까지, 하나의 로봇카로 3단계 성장하는 AI 교육"

### Features

| 아이콘 | 라벨 | 설명 |
|--------|------|------|
| 📷 | 얼굴 인식 | OpenCV + Haar Cascade |
| 🔐 | AI 도어락 | Face Recognition 시스템 |
| 🚗 | YOLO 자율주행 | 객체 인식 + 차선 트레이싱 |
| 🐧 | Linux 개발 | VSCode + VNC + FileZilla |

---

## 📊 Course Info

| 항목 | 아이콘 | 색상 | 내용 |
|------|--------|------|------|
| 수업 시간 | ⏰ Clock | purple | 3시간 / 6시간 / 12시간 선택 |
| 수강 인원 | 👥 Users | blue | 최대 20명 (2인 1팀 권장) |
| 준비물 | 🤖 Robot | green | 라즈베리파이 4 로봇카 1종 (전 과정 동일) |
| 수업 방식 | 🎯 Target | orange | 벤치마킹 → 메이커 활동 → AI 바이브 코딩 |

---

## 📖 과정 소개

### 타이틀
"왜 하나의 라즈베리파이 로봇카로 3단계 학습인가?"

### 내용

**학교 예산을 고려한 실용적 AI 비전 교육 설계**

라즈베리파이 4 로봇카 **단 1종**으로 얼굴 인식부터 YOLO 자율주행까지 단계별로 성장하는 커리큘럼입니다. 학교나 학원에서 한정된 예산으로도 고품질 AI 비전 교육이 가능하도록 설계했습니다.

**3단계 성장 구조**
- **1단계 (3시간)**: OpenCV로 실시간 얼굴 인식 카메라 제작
- **2단계 (6시간)**: Face Recognition으로 AI 도어락 시스템 개발
- **3단계 (12시간)**: YOLO로 객체 인식 자율주행 로봇카 완성

**Linux 실전 개발 환경**

라즈베리파이는 **Linux OS**가 실행되는 초소형 컴퓨터로, ESP32와 달리 **Python 패키지를 직접 설치**하고 **딥러닝 모델을 로컬에서 실행**할 수 있습니다.

- **VNC Viewer**: 원격 데스크톱으로 라즈베리파이 화면 제어
- **VSCode Remote SSH**: 코드 편집 및 디버깅
- **FileZilla**: 파일 전송 (SFTP)
- **Linux 명령어**: `sudo`, `apt`, `pip`, `chmod` 등 실전 활용

**AI 바이브 코딩 중심 교육**

벤치마킹으로 시작해 현재 수준을 파악하고, 메이커 활동을 통해 하드웨어를 조립한 후, **AI 도구로 코드를 생성·수정**하며 개발 프로세스와 알고리즘을 학습합니다.

**라즈베리파이 자체에서 모든 처리**

Flask 서버, OpenCV, YOLO 모두 **라즈베리파이 내부**에서 실행되므로 외부 서버나 인터넷 연결이 불필요합니다. 학생이 직접 `pip install`로 패키지를 설치하고 관리하며 실전 개발 경험을 쌓습니다.

### 이미지 (3개)
- 라즈베리파이 4 로봇카 완성 모습
- 학생들이 VNC로 원격 제어하는 장면
- YOLO 객체 인식 자율주행 시연

---

## 🎓 학습 경로 (Learning Path)

### 전체 구조도: 하나의 교구재, 3단계 성장

### 3시간 과정: 얼굴 인식 카메라 (1단계)

**목표**: OpenCV로 실시간 얼굴 인식 시스템 완성

| 단계 | 시간 | 내용 | 교육 방법 |
|------|------|------|----------|
| 1. 벤치마킹 | 15분 | 테슬라·스마트홈 AI 비전 사례 | 영상 시청 + 토론 |
| 2. 현재 수준 파악 | 15분 | 완성된 얼굴 인식 체험 (Predict) | 직접 체험 |
| 3. 개발환경 구축 | 30분 | VNC + VSCode + FileZilla 설정 | Linux 명령어 실습 |
| 4. OpenCV 실행 | 25분 | 얼굴 인식 코드 실행 | Python + pip |
| 5. 알고리즘 이해 | 20분 | Haar Cascade 원리 (Investigate) | 이론 설명 |
| 6. 개선 활동 | 45분 | 색상, 필터, 카운터 추가 | AI 바이브 코딩 |
| 7. 발표 | 10분 | 실시간 시연 | 🎉 |

**완성 작품**: 실시간 얼굴 인식 카메라 (OpenCV + Haar Cascade)  
**핵심 역량**: Linux 환경, Python 개발, OpenCV

### 6시간 과정: 얼굴 인식 + AI 도어락 (1-2단계)

**목표**: 얼굴 인식 + Face Recognition 도어락 시스템 완성

| 세션 | 시간 | 프로젝트 | 핵심 기술 |
|------|------|----------|-----------|
| 1교시 | 90분 | 개발환경 구축 | VNC, VSCode, Linux 명령어 |
| 2교시 | 90분 | 얼굴 인식 카메라 | OpenCV + Haar Cascade |
| 3교시 | 90분 | Face Recognition | 얼굴 등록 + 인증 알고리즘 |
| 4교시 | 90분 | AI 도어락 통합 | GPIO 제어 (서보, LED, 부저) |

**완성 작품**: 얼굴 인식 + AI 도어락 시스템 (동일 교구재)  
**핵심 역량**: Face Recognition, GPIO 제어, 실전 배포

### 12시간 과정: 얼굴 인식 + AI 도어락 + YOLO 자율주행 (1-2-3단계 완성)

**목표**: YOLO 객체 인식 자율주행 로봇카 최종 완성

**일정표**

| 차시 | 시간 | 단계 | 프로젝트 | 핵심 기술 |
|------|------|------|----------|-----------|
| 1차시 | 2시간 | 얼굴 인식 1 | VNC + VSCode + Linux 명령어 | 개발환경 |
| 2차시 | 2시간 | 얼굴 인식 2 | OpenCV + Haar Cascade | 이미지 처리 |
| 3차시 | 2시간 | AI 도어락 1 | Face Recognition 얼굴 등록+인증 | 딥러닝 |
| 4차시 | 2시간 | AI 도어락 2 | GPIO (서보+LED+부저) | 하드웨어 제어 |
| 5차시 | 2시간 | YOLO 자율주행 1 | YOLOv5 설치 + 객체 탐지 | YOLO 모델 |
| 6차시 | 2시간 | YOLO 자율주행 2 | 차선 인식 + 모터 제어 통합 | 로봇카 제어 |

**완성 작품**: 3단계 통합 AI 비전 로봇카 + 개인 포트폴리오  
**핵심 역량**: 전체 AI 개발 파이프라인 (Linux → OpenCV → YOLO)

---

## 🎮 프로젝트 상세

### 1단계: OpenCV 얼굴 인식 카메라 (3시간 과정)

**난이도**: ⭐⭐ (초급)  
**소요 시간**: 3시간  
**대상**: 중등 2학년 ~ 고등 3학년

**학습 목표**
- 라즈베리파이 Linux 환경 이해
- VNC, VSCode, FileZilla 사용법
- OpenCV 설치 및 이미지 처리
- Haar Cascade 얼굴 검출 알고리즘

**제작 단계**

| 단계 | 내용 | 시간 | 방법 |
|------|------|------|------|
| 1. 벤치마킹 | AI 비전 사례 분석 | 15분 | 영상 + 토론 |
| 2. 환경 구축 | VNC + VSCode Remote SSH | 30분 | Linux 명령어 실습 |
| 3. 패키지 설치 | OpenCV pip 설치 | 25분 | `pip3 install opencv-python` |
| 4. 코드 실행 | 얼굴 인식 예제 실행 | 25분 | Python 스크립트 |
| 5. 알고리즘 이해 | Haar Cascade 원리 | 20분 | 이론 설명 |
| 6. 개선 활동 | 색상, 카운터, 필터 추가 | 45분 | AI 바이브 코딩 |
| 7. 발표 | 실시간 시연 | 10분 | 🎉 |

**개발환경 구축: Linux 명령어 실습**

```bash
# 1. 라즈베리파이 SSH 접속
ssh pi@192.168.0.10
# 기본 비밀번호: raspberry

# 2. 시스템 업데이트
sudo apt update
sudo apt upgrade -y

# 3. Python 패키지 설치
sudo apt install python3-pip -y
pip3 install opencv-python
pip3 install numpy

# 4. 카메라 활성화 확인
vcgencmd get_camera
# supported=1 detected=1 확인

# 5. 테스트 사진 촬영
raspistill -o test.jpg

# 6. 작업 디렉토리 생성
mkdir ~/ai_vision
cd ~/ai_vision

# 7. 파일 권한 설정
chmod +x face_detection.py
```

**VSCode Remote SSH 연결**

| 도구 | 용도 | 설정 |
|------|------|------|
| VNC Viewer | GUI 원격 데스크톱 | `192.168.0.10:5900` |
| VSCode | 코드 편집 + 디버깅 | Remote SSH 확장 |
| FileZilla | 파일 전송 (SFTP) | Port 22, pi 계정 |

**핵심 알고리즘: Haar Cascade 얼굴 검출**

```python
import cv2

# Haar Cascade 모델 로드 (라즈베리파이 로컬)
face_cascade = cv2.CascadeClassifier(
    '/usr/share/opencv4/haarcascades/haarcascade_frontalface_default.xml'
)

# 카메라 초기화 (라즈베리파이 카메라 모듈)
cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

face_count = 0

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    
    # 얼굴 검출 (핵심 알고리즘)
    faces = face_cascade.detectMultiScale(
        gray, 
        scaleFactor=1.1,   # 이미지 피라미드 스케일
        minNeighbors=5,    # 최소 이웃 개수
        minSize=(30, 30)   # 최소 얼굴 크기
    )
    
    # 얼굴에 사각형 그리기
    for i, (x, y, w, h) in enumerate(faces):
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
        cv2.putText(frame, f"Person {i+1}", (x, y-10), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
    
    # 좌상단에 총 인원 표시
    cv2.putText(frame, f"Total: {len(faces)}", (10, 30), 
                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)
    
    cv2.imshow('Face Detection', frame)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

**실행 방법**

```bash
# 라즈베리파이에서 직접 실행
cd ~/ai_vision
python3 face_detection.py

# 백그라운드 실행
nohup python3 face_detection.py &

# 프로세스 확인
ps aux | grep python

# 종료
pkill -f face_detection.py
```

**기대 효과**
- ✅ Linux 환경 실전 경험 (ssh, apt, pip)
- ✅ 원격 개발 환경 구축 (VNC, VSCode)
- ✅ OpenCV 실시간 이미지 처리
- ✅ 라즈베리파이 자체 실행 (외부 서버 불필요)

### 2단계: Face Recognition AI 도어락 (6시간 과정)

**난이도**: ⭐⭐⭐ (중급)  
**소요 시간**: 6시간 (얼굴 인식 3h + AI 도어락 3h)  
**대상**: 중등 3학년 ~ 고등 3학년

**시스템 구성**

| 모듈 | 역할 | 연결 |
|------|------|------|
| 라즈베리파이 4 | 메인 컨트롤러 + Python 실행 | - |
| 카메라 모듈 v2 | 얼굴 촬영 | CSI 포트 |
| 서보모터 (SG90) | 도어락 제어 | GPIO 18 (PWM) |
| LED (녹색) | 인증 성공 | GPIO 23 |
| LED (빨간색) | 인증 실패 | GPIO 24 |
| 부저 | 경고음 | GPIO 25 |

**패키지 설치 (라즈베리파이 내부)**

```bash
# Face Recognition 설치 (약 10-15분 소요)
pip3 install face_recognition
pip3 install RPi.GPIO

# 의존성 설치
sudo apt install libatlas-base-dev -y
sudo apt install libopenblas-dev -y
```

**제어 로직**

```python
import cv2
import face_recognition
import RPi.GPIO as GPIO

# 서보모터 설정
SERVO_PIN = 18
GPIO.setmode(GPIO.BCM)
GPIO.setup(SERVO_PIN, GPIO.OUT)
pwm = GPIO.PWM(SERVO_PIN, 50)  # 50Hz
pwm.start(0)

# 등록된 얼굴 로드
known_face_encodings = []
known_face_names = []
# ... 얼굴 데이터 로드 ...

cap = cv2.VideoCapture(0)

def open_door():
    pwm.ChangeDutyCycle(7.5)  # 90도 회전
    GPIO.output(LED_GREEN, GPIO.HIGH)
    time.sleep(3)
    pwm.ChangeDutyCycle(2.5)  # 0도 (닫힘)
    GPIO.output(LED_GREEN, GPIO.LOW)

while True:
    ret, frame = cap.read()
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    
    # 얼굴 검출
    face_locations = face_recognition.face_locations(rgb_frame)
    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)
    
    for face_encoding in face_encodings:
        # 얼굴 인식 (핵심 알고리즘)
        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)
        name = "Unknown"
        
        if True in matches:
            first_match_index = matches.index(True)
            name = known_face_names[first_match_index]
            
            # 인증 성공 → 도어 오픈
            open_door()
            log_access(name, True)
        else:
            # 인증 실패 → 경고
            GPIO.output(LED_RED, GPIO.HIGH)
            GPIO.output(BUZZER, GPIO.HIGH)
            time.sleep(0.5)
            GPIO.output(LED_RED, GPIO.LOW)
            GPIO.output(BUZZER, GPIO.LOW)
            log_access("Unknown", False)
    
    cv2.imshow('Door Lock', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
```

**기능 확장**

| 기능 | 설명 | 난이도 |
|------|------|--------|
| 웹 대시보드 | Flask로 접근 기록 조회 | ⭐⭐ |
| 스마트폰 알림 | Telegram 봇 연동 | ⭐⭐⭐ |
| 다중 사용자 | 얼굴 여러 명 등록 | ⭐⭐ |
| 음성 안내 | "환영합니다" TTS | ⭐⭐ |

### 3단계: YOLOv5 객체 인식 자율주행 로봇카 (12시간 과정)

**난이도**: ⭐⭐⭐⭐⭐ (고급)  
**소요 시간**: 12시간 (얼굴 인식 4h + AI 도어락 4h + YOLO 자율주행 4h)  
**대상**: 고등 1-3학년

**시스템 구조**

**YOLOv5 설치 (라즈베리파이 내부)**

```bash
# 1. PyTorch 경량화 버전 설치 (라즈베리파이용)
pip3 install torch torchvision torchaudio

# 2. YOLOv5 설치
pip3 install yolov5

# 3. 사전 학습된 모델 다운로드
cd ~/ai_vision
wget https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5s.pt

# 4. 테스트
python3 -c "import torch; print(torch.__version__)"
```

**하드웨어 구성 (로봇카)**

| 부품 | 역할 | 연결 |
|------|------|------|
| 라즈베리파이 4 | 메인 컨트롤러 | - |
| 카메라 모듈 v2 | 영상 촬영 | CSI |
| L298N 모터 드라이버 | DC 모터 제어 | GPIO 17, 27, 22, 23 |
| DC 모터 4개 | 4WD 구동 | L298N |
| 초음파 센서 HC-SR04 | 거리 측정 | GPIO 5, 6 |
| 배터리 (7.4V) | 전원 | L298N + Pi |

**주요 알고리즘**

**1) 차선 인식**

```python
def detect_lane(frame):
    # 1. 관심 영역 (ROI) 설정
    height, width = frame.shape[:2]
    roi_vertices = [(0, height), (width/2, height/2), (width, height)]
    mask = np.zeros_like(frame)
    cv2.fillPoly(mask, np.array([roi_vertices], dtype=np.int32), 255)
    masked = cv2.bitwise_and(frame, mask)
    
    # 2. 엣지 검출
    gray = cv2.cvtColor(masked, cv2.COLOR_BGR2GRAY)
    blur = cv2.GaussianBlur(gray, (5, 5), 0)
    edges = cv2.Canny(blur, 50, 150)
    
    # 3. 허프 변환으로 직선 검출
    lines = cv2.HoughLinesP(edges, 1, np.pi/180, 50, maxLineGap=50)
    
    # 4. 좌우 차선 구분
    left_lines = []
    right_lines = []
    for line in lines:
        x1, y1, x2, y2 = line[0]
        slope = (y2 - y1) / (x2 - x1)
        if slope < 0:  # 왼쪽 차선
            left_lines.append(line)
        else:  # 오른쪽 차선
            right_lines.append(line)
    
    return left_lines, right_lines
```

**2) YOLOv5 객체 탐지 (라즈베리파이 로컬 실행)**

```python
import torch
import cv2
import numpy as np

# YOLOv5 모델 로드 (라즈베리파이 내부)
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
model.conf = 0.5  # 신뢰도 임계값

# 또는 로컬에 다운로드한 모델 사용
# model = torch.load('yolov5s.pt')

def detect_objects(frame):
    """
    YOLOv5로 객체 탐지
    Returns: 검출된 객체 리스트 (클래스, 좌표, 신뢰도)
    """
    # RGB로 변환 (YOLOv5는 RGB 입력)
    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    
    # YOLO 추론 (라즈베리파이에서 약 0.5-1초 소요)
    results = model(rgb_frame)
    
    # 결과 파싱
    detections = results.pandas().xyxy[0]  # Pandas DataFrame
    
    objects = []
    for idx, row in detections.iterrows():
        obj = {
            'class': row['name'],
            'confidence': row['confidence'],
            'bbox': [int(row['xmin']), int(row['ymin']), 
                     int(row['xmax']), int(row['ymax'])]
        }
        objects.append(obj)
    
    return objects

def draw_detections(frame, objects):
    """검출 결과를 화면에 표시"""
    for obj in objects:
        x1, y1, x2, y2 = obj['bbox']
        label = f"{obj['class']}: {obj['confidence']:.2f}"
        
        # 바운딩 박스
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        
        # 라벨
        cv2.putText(frame, label, (x1, y1-10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
    
    return frame

# 실시간 객체 인식
cap = cv2.VideoCapture(0)

while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # 객체 탐지
    objects = detect_objects(frame)
    
    # 결과 표시
    frame = draw_detections(frame, objects)
    
    cv2.imshow('YOLOv5 Detection', frame)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

**성능 최적화 (라즈베리파이 4 기준)**

```python
# 해상도 낮추기 (FPS 향상)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)

# 프레임 스킵 (2프레임마다 1번 처리)
frame_count = 0
while True:
    ret, frame = cap.read()
    frame_count += 1
    
    if frame_count % 2 == 0:
        objects = detect_objects(frame)
        frame = draw_detections(frame, objects)
    
    cv2.imshow('YOLOv5', frame)
```

**3) 자율주행 의사결정 (라즈베리파이 통합)**

```python
import RPi.GPIO as GPIO
import time

# GPIO 설정 (모터 제어)
MOTOR_LEFT_PWM = 17
MOTOR_LEFT_DIR = 27
MOTOR_RIGHT_PWM = 22
MOTOR_RIGHT_DIR = 23

GPIO.setmode(GPIO.BCM)
GPIO.setup(MOTOR_LEFT_PWM, GPIO.OUT)
GPIO.setup(MOTOR_LEFT_DIR, GPIO.OUT)
GPIO.setup(MOTOR_RIGHT_PWM, GPIO.OUT)
GPIO.setup(MOTOR_RIGHT_DIR, GPIO.OUT)

left_pwm = GPIO.PWM(MOTOR_LEFT_PWM, 1000)  # 1kHz
right_pwm = GPIO.PWM(MOTOR_RIGHT_PWM, 1000)
left_pwm.start(0)
right_pwm.start(0)

def motor_forward(speed):
    """전진 (속도: 0-100)"""
    GPIO.output(MOTOR_LEFT_DIR, GPIO.HIGH)
    GPIO.output(MOTOR_RIGHT_DIR, GPIO.HIGH)
    left_pwm.ChangeDutyCycle(speed)
    right_pwm.ChangeDutyCycle(speed)

def motor_stop():
    """정지"""
    left_pwm.ChangeDutyCycle(0)
    right_pwm.ChangeDutyCycle(0)

def motor_steer(angle):
    """조향 (-100 ~ 100)"""
    if angle < 0:  # 좌회전
        left_pwm.ChangeDutyCycle(50 + angle)
        right_pwm.ChangeDutyCycle(50)
    else:  # 우회전
        left_pwm.ChangeDutyCycle(50)
        right_pwm.ChangeDutyCycle(50 - angle)

def autonomous_drive():
    """통합 자율주행 알고리즘"""
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 320)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 240)
    
    frame_count = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        frame_count += 1
        
        # 1. 차선 인식 (매 프레임)
        left_lane, right_lane = detect_lane(frame)
        steering = calculate_steering(left_lane, right_lane)
        
        # 2. YOLO 객체 탐지 (2프레임마다)
        obstacles = []
        if frame_count % 2 == 0:
            objects = detect_objects(frame)
            
            # 전방 장애물 확인
            for obj in objects:
                if obj['class'] in ['person', 'car', 'bicycle']:
                    x1, y1, x2, y2 = obj['bbox']
                    # 화면 하단 40% 영역
                    if y2 > frame.shape[0] * 0.6:
                        obstacles.append(obj['class'])
        
        # 3. 초음파 센서
        distance = get_ultrasonic_distance()
        
        # 4. 의사결정
        if distance < 30 or obstacles:
            # 장애물 감지 → 정지
            motor_stop()
            print(f"STOP: Obstacle detected - {obstacles}, distance: {distance}cm")
        elif abs(steering) > 40:
            # 급커브 → 감속
            motor_forward(speed=40)
            motor_steer(steering)
            print(f"TURN: steering={steering}")
        else:
            # 정상 주행
            motor_forward(speed=70)
            motor_steer(steering)
            print(f"FORWARD: steering={steering}")
        
        cv2.imshow('Autonomous Robot Car', frame)
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()
    GPIO.cleanup()

# 실행
if __name__ == '__main__':
    try:
        autonomous_drive()
    except KeyboardInterrupt:
        GPIO.cleanup()
```

**실행 방법 (라즈베리파이)**

```bash
# 1. 코드 작성 (VSCode Remote SSH)
# 2. 권한 부여
chmod +x autonomous_car.py

# 3. 실행
sudo python3 autonomous_car.py

# 4. 백그라운드 실행
nohup sudo python3 autonomous_car.py > robot.log 2>&1 &

# 5. 로그 확인
tail -f robot.log

# 6. 종료
sudo pkill -f autonomous_car.py
```

**완성 기능**

| 기능 | 설명 | 정확도 |
|------|------|--------|
| 차선 유지 | Canny + Hough Transform | 85% |
| 객체 탐지 | YOLOv4-tiny | 90% |
| 장애물 회피 | 초음파 + YOLO 융합 | 95% |
| 표지판 인식 | CNN 분류기 | 80% |
| 주차 | 라인 트래킹 | 90% |

---

## 📚 커리큘럼 상세

### 3시간 과정: OpenCV 얼굴 인식

**목표**: OpenCV로 실시간 얼굴 인식 카메라 완성

**시간표**

| 시간 | 내용 | 활동 | 산출물 |
|------|------|------|--------|
| 00:00-00:15 | 벤치마킹 | AI 비전 사례 분석 | 토론 노트 |
| 00:15-00:30 | 현재 수준 파악 | 완성 프로그램 체험 (Predict) | 체험 리포트 |
| 00:30-01:00 | 개발환경 구축 | VNC + VSCode + Linux 명령어 | 연결 성공 |
| 01:00-01:25 | OpenCV 실행 | pip install + 코드 실행 | 얼굴 인식 작동 |
| 01:25-01:45 | 알고리즘 이해 | Haar Cascade 원리 (Investigate) | 이해도 체크 |
| 01:45-02:30 | 개선 활동 | 색상, 카운터, 필터 추가 (Modify) | AI 바이브 코딩 |
| 02:30-02:50 | 확장 | 나이/성별 추정 (Make) | 완성 작품 |
| 02:50-03:00 | 발표 | 실시간 시연 | 🎉 시연 영상 |

**준비물**
- 라즈베리파이 4 로봇카 (2인 1세트)
- 노트북 (VNC Viewer + VSCode)
- WiFi 공유기 (라즈베리파이 연결용)

**제공 자료**
- 라즈베리파이 OS 이미지 (사전 설치)
- Python 예제 코드 (GitHub)
- VNC/VSCode 설정 가이드

### 6시간 과정: 얼굴 인식 + AI 도어락

**목표**: OpenCV 얼굴 인식 + Face Recognition 도어락 시스템

**일정 구성**

**세부 시간표**

| 교시 | 시간 | 프로젝트 | 세부 활동 |
|------|------|----------|-----------|
| **1교시** | **09:00-10:30** | **개발환경 + 얼굴 인식** | |
| | 09:00-09:15 | 벤치마킹 | AI 비전 사례 분석 |
| | 09:15-09:45 | 환경 구축 | VNC + VSCode + Linux |
| | 09:45-10:15 | OpenCV 실행 | pip install + 코드 실행 |
| | 10:15-10:30 | Haar Cascade | 알고리즘 이해 |
| **2교시** | **10:30-12:00** | **얼굴 인식 개선** | |
| | 10:30-11:15 | Modify | 색상, 카운터 추가 |
| | 11:15-12:00 | Make | AI 바이브 코딩 |
| **점심** | **12:00-13:00** | 휴식 | - |
| **3교시** | **13:00-14:30** | **Face Recognition** | |
| | 13:00-13:30 | 패키지 설치 | pip3 install face_recognition |
| | 13:30-14:00 | 얼굴 등록 | 사진 촬영 + 인코딩 |
| | 14:00-14:30 | 인증 시스템 | 실시간 인식 |
| **4교시** | **14:30-16:00** | **GPIO 도어락** | |
| | 14:30-15:00 | GPIO 기초 | LED, 부저 테스트 |
| | 15:00-15:30 | 서보모터 | PWM 제어 |
| | 15:30-16:00 | 통합 & 발표 | AI 도어락 시연 |

### 12시간 과정: OpenCV + Face Recognition + YOLO 완전 정복

**목표**: 얼굴 인식 + AI 도어락 + YOLO 자율주행 로봇카

**전체 구조**

**일차별 계획**

| 일차 | 주제 | 핵심 기술 | 시간 배분 |
|------|------|-----------|-----------|
| **1일차** | **얼굴 인식** | **Linux + OpenCV** | **4시간** |
| | 벤치마킹 | AI 비전 사례 분석 | 20분 |
| | 개발환경 구축 | VNC + VSCode + FileZilla | 70분 |
| | Linux 명령어 | ssh, apt, pip, chmod | 30분 |
| | OpenCV 실행 | Haar Cascade 얼굴 인식 | 60분 |
| | 개선 활동 | AI 바이브 코딩 | 60분 |
| **2일차** | **AI 도어락** | **Face Recognition + GPIO** | **4시간** |
| | 1일차 복습 | 얼굴 인식 리뷰 | 15분 |
| | Face Recognition | pip install + 얼굴 등록 | 60분 |
| | 인증 시스템 | 실시간 인식 + 인증 | 60분 |
| | GPIO 제어 | 서보모터 + LED + 부저 | 60분 |
| | 통합 시스템 | AI 도어락 완성 | 45분 |
| **3일차** | **YOLO 자율주행** | **YOLOv5 + 로봇카** | **4시간** |
| | 2일차 복습 | AI 도어락 리뷰 | 15분 |
| | YOLOv5 설치 | pip install torch yolov5 | 45분 |
| | 객체 탐지 | YOLO 실시간 추론 | 60분 |
| | 모터 제어 | GPIO PWM + L298N | 45분 |
| | 자율주행 통합 | 차선 + 객체 + 초음파 | 60분 |
| | 포트폴리오 | 3단계 정리 & 발표 | 15분 |

**세부 일정 (예시: 3일차 YOLO 자율주행)**

| 시간 | 활동 | 내용 | 산출물 |
|------|------|------|--------|
| 09:00-09:15 | 복습 | • 얼굴 인식 확인<br/>• AI 도어락 확인<br/>• 3일 로드맵 | - |
| 09:15-10:00 | YOLOv5 설치 | • pip3 install torch<br/>• pip3 install yolov5<br/>• 모델 다운로드 | YOLOv5 작동 |
| 10:00-11:00 | 객체 탐지 | • 실시간 YOLO 추론<br/>• 사람/차 인식<br/>• 바운딩 박스 | 객체 인식 |
| 11:00-11:45 | 모터 제어 | • L298N 연결<br/>• GPIO PWM 제어<br/>• 전진/후진/회전 | 로봇카 제어 |
| 11:45-12:00 | 1차 테스트 | • 수동 주행 테스트 | - |
| 12:00-13:00 | 점심 | - | - |
| 13:00-14:00 | 자율주행 통합 | • 차선 인식<br/>• YOLO + 초음파 융합<br/>• 의사결정 알고리즘 | 자율주행 로봇카 |
| 14:00-14:15 | 포트폴리오 | • 3단계 정리<br/>• 발표 준비 | PPT |
| 14:15-14:30 | 최종 발표 | • 개인별 시연<br/>• 질의응답 | 시연 영상 |

---

## 💰 가격 정책

### 3시간 과정: 얼굴 인식 카메라

| 항목 | 기본 | 프리미엄 |
|------|------|----------|
| 수업 시간 | 3시간 | 3시간 |
| 수강 인원 | 최대 20명 (2인 1팀) | 최대 20명 (1인 1팀) |
| 강사 | 1명 | 2명 (메인+Linux 보조) |
| 키트 | 라즈베리파이 4 로봇카 (공용) | 라즈베리파이 4 로봇카 (개인) |
| 제공 자료 | 디지털 (GitHub) | 디지털 + USB + 노트북 |
| 완성 작품 | 체험 (반납) | 개인 소유 |
| 사후 지원 | - | 1주일 온라인 질문 |
| **가격** | **50만원** | **80만원 + 키트비** |

※ 키트비: 라즈베리파이 4 (4GB) + 카메라 + 센서 = 7만원  
※ 교구재 단일화로 유지보수 편리

### 6시간 과정: 얼굴 인식 + AI 도어락

| 항목 | 기본 | 프리미엄 |
|------|------|----------|
| 수업 시간 | 6시간 (1일) | 6시간 (1일) |
| 수강 인원 | 최대 20명 (2인 1팀) | 최대 20명 (1인 1팀) |
| 강사 | 2명 (Linux+AI) | 3명 (Linux+AI+보조) |
| 키트 | 라즈베리파이 4 로봇카 (공용) | 라즈베리파이 4 로봇카 (개인) |
| 제공 자료 | 디지털 (코드+모델) | 풀패키지 + USB |
| 완성 작품 | 2단계 (반납) | 2단계 (개인 소유) |
| 사후 지원 | 1주일 | 2주일 + 화상 피드백 |
| 추가 혜택 | - | • 정식 과정 20% 할인<br/>• VSCode 설정 가이드 |
| **가격** | **90만원** | **130만원 + 키트비** |

※ 키트비: 라즈베리파이 4 로봇카 1세트 7만원  
※ 동일 교구재로 2단계 학습 (비용 효율적)

### 12시간 과정: 얼굴 인식 + AI 도어락 + YOLO 자율주행

| 항목 | 기본 | 프리미엄 |
|------|------|----------|
| 수업 시간 | 12시간 (3일, 각 4시간) | 12시간 (3일, 각 4시간) |
| 수강 인원 | 최대 20명 (2인 1팀) | 최대 20명 (1인 1팀) |
| 강사 | 2명 (Linux+AI 전문가) | 3명 (Linux 2 + AI 1) |
| 키트 | 라즈베리파이 4 로봇카 (공용) | 라즈베리파이 4 로봇카 (개인) |
| 제공 자료 | 디지털 + YOLO 모델 | 풀패키지 + USB + 노트북 |
| 완성 작품 | 3단계 통합 (반납) | 3단계 통합 (개인 소유) |
| 사후 지원 | 2주일 | 1개월 + 월 1회 화상 |
| 추가 혜택 | 정식 과정 10% 할인 | • 정식 과정 30% 할인<br/>• 포트폴리오 웹사이트<br/>• AI 대회 참가 지원<br/>• GitHub 리포지토리 |
| **가격** | **180만원** | **250만원 + 키트비** |

※ 키트비: 라즈베리파이 4 로봇카 1세트 7만원  
※ 1종 교구재로 3단계 완성 (최대 비용 절감)

---

## 🎯 교육 효과

### 학습 성과

### 역량 성장 비교

| 역량 | 3시간 | 6시간 | 12시간 |
|------|-------|-------|--------|
| Linux 환경 | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| OpenCV | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| 딥러닝 AI | - | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| GPIO 제어 | - | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| 개발 프로세스 | ⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| 실전 배포 | ⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |

### 학생 반응

**3시간 과정 후:**
> "Linux 명령어를 직접 쳐서 OpenCV를 설치하니 진짜 개발자가 된 기분이에요!" - 중등 2학년

> "VNC로 라즈베리파이를 원격 제어하는 게 신기해요. 집에서도 해보고 싶어요" - 중등 3학년

**6시간 과정 후:**
> "제 얼굴을 인식해서 도어락이 열리는 게 정말 멋있어요. 우리 집에 설치하고 싶어요" - 고등 1학년

> "GPIO로 서보모터를 제어하니 하드웨어와 소프트웨어가 연결되는 게 느껴져요" - 고등 2학년

**12시간 과정 후:**
> "YOLO로 사람과 차를 인식하고 피하는 로봇카를 만들었어요. 대회에 나가고 싶어요" - 고등 2학년

> "하나의 로봇카로 3단계를 다 배우니 발전하는 게 느껴져요. 포트폴리오로 만들 거예요" - 고등 3학년

---

## 📞 문의

**홈페이지**: https://aimakerlab.com  
**이메일**: trial-raspberry@aimakerlab.com  
**전화**: 02-XXXX-XXXX

---

## 📝 문서 정보

**최종 업데이트**: 2026-01-03  
**버전**: 2.0 (교구재 단일화 + 라즈베리파이 로컬 실행)  
**작성자**: AI Maker Lab 교육팀  
**주요 변경사항**:
- 교구재를 라즈베리파이 4 로봇카 1종으로 통일 (학교 예산 고려)
- 얼굴 인식(OpenCV) → AI 도어락(Face Recognition) → YOLO 자율주행 3단계 구조
- 라즈베리파이 내부에서 Python 로컬 실행 (외부 서버 불필요)
- Linux 실전 개발 환경: VNC + VSCode Remote SSH + FileZilla
- AI 바이브 코딩 중심 교육 방법론 적용

**다음 단계**: JSON 변환 → 웹사이트 적용 → 라즈베리파이 이미지 제작

---

## 💡 추가 정보

### 교구재 단일화의 장점

**학교/학원 입장**
- ✅ 예산 절감: 1종 교구재로 3단계 학습
- ✅ 관리 용이: 동일 하드웨어 유지보수
- ✅ 확장 가능: 같은 로봇카로 추가 AI 프로젝트
- ✅ 재사용: 다음 학기에도 동일 키트 활용

**학생 입장**
- ✅ 성장 실감: 같은 교구재가 단계별로 진화
- ✅ 익숙함: 하드웨어 구조 반복 학습
- ✅ Linux 경험: 실제 개발 환경 체험
- ✅ 포트폴리오: 3단계 발전 과정 기록

### 기술 스택 정리

| 단계 | 하드웨어 | 소프트웨어 | AI/ML |
|------|----------|-----------|-------|
| 1단계 | 라즈베리파이 4 + 카메라 | Python + OpenCV | Haar Cascade |
| 2단계 | 동일 + GPIO (서보, LED) | Python + face_recognition | 얼굴 인식 딥러닝 |
| 3단계 | 동일 + 모터 드라이버 | Python + YOLOv5 | 객체 인식 딥러닝 |

### 개발환경 도구

| 도구 | 용도 | 설치 방법 |
|------|------|-----------|
| VNC Viewer | GUI 원격 데스크톱 | 클라이언트 PC에 설치 |
| VSCode Remote SSH | 코드 편집 + 디버깅 | VSCode 확장 설치 |
| FileZilla | 파일 전송 (SFTP) | 클라이언트 PC에 설치 |
| Raspberry Pi Imager | OS 이미지 구워넣기 | 공식 사이트 다운로드 |

### FAQ

**Q1. Flask 서버가 왜 필요 없나요?**
- A: 라즈베리파이가 Linux를 실행하므로 자체적으로 Python을 구동할 수 있습니다. ESP32처럼 외부 서버가 필요 없습니다.

**Q2. 라즈베리파이 4가 YOLO를 실행할 수 있나요?**
- A: YOLOv5 경량 모델(yolov5s)을 사용하면 약 1-2 FPS로 실행 가능합니다. 실시간은 아니지만 교육용으로 충분합니다.

**Q3. VNC 없이 모니터로 연결할 수 있나요?**
- A: 가능합니다. HDMI 모니터 + USB 키보드/마우스를 연결하면 일반 PC처럼 사용할 수 있습니다.

**Q4. 라즈베리파이 OS는 어떻게 설치하나요?**
- A: Raspberry Pi Imager로 microSD 카드에 Raspberry Pi OS를 설치합니다. 사전 설정된 이미지를 제공합니다.

**Q5. GPU가 없는데 딥러닝이 가능한가요?**
- A: 라즈베리파이는 CPU만 있지만, 경량 모델과 프레임 스킵 기법으로 실시간에 가까운 추론이 가능합니다.

**Q6. ESP32와 라즈베리파이의 차이는?**
- A: ESP32는 마이크로컨트롤러(펌웨어 업로드), 라즈베리파이는 초소형 컴퓨터(Linux OS 실행)입니다. 라즈베리파이가 더 강력하지만 비용이 높습니다.

**Q7. 3단계를 건너뛸 수 있나요?**
- A: 권장하지 않습니다. 1단계에서 Linux 환경을, 2단계에서 GPIO 제어를 익혀야 3단계 자율주행을 이해할 수 있습니다.

